{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tk_SWubkYRlc"
   },
   "source": [
    "## used to be inspired from: \n",
    "https://medium.com/@anthony.galtier/fine-tuning-bert-for-a-regression-task-is-a-description-enough-to-predict-a-propertys-list-price-cf97cd7cb98a\n",
    "\n",
    "Since we changed to classification it is freestyle now :)\n",
    "\n",
    "https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5EGn9EGnBgK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnRWHW2NsrFg"
   },
   "outputs": [],
   "source": [
    "X_train = pickle.load(open(\"drive/MyDrive/ABA project/X_train.p\", \"rb\"))\n",
    "X_val =  pickle.load(open(\"drive/MyDrive/ABA project/X_val.p\", \"rb\"))\n",
    "X_test =  pickle.load(open(\"drive/MyDrive/ABA project/X_test.p\", \"rb\"))\n",
    "y_train = pickle.load(open(\"drive/MyDrive/ABA project/y_train.p\", \"rb\"))\n",
    "y_val = pickle.load(open(\"drive/MyDrive/ABA project/y_val.p\", \"rb\"))\n",
    "y_test = pickle.load(open(\"drive/MyDrive/ABA project/y_test.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jhdxejeMkvyo"
   },
   "outputs": [],
   "source": [
    "X_train_source = np.array(pd.get_dummies(X_train['source_name']))\n",
    "X_val_source = np.array(pd.get_dummies(X_val['source_name']))\n",
    "X_test_source = np.array(pd.get_dummies(X_test['source_name']))\n",
    "\n",
    "X_train_image = np.array(X_train['filepath'])\n",
    "X_val_image = np.array(X_val['filepath'])\n",
    "X_test_image = np.array(X_test['filepath'])\n",
    "\n",
    "X_train_text = list(X_train['title'] + X_train['description'])\n",
    "X_val_text = list(X_val['title'] + X_val['description'])\n",
    "X_test_text = list(X_test['title'] + X_test['description'])\n",
    "y_train = list(np.log1p(y_train['engagement']))\n",
    "y_val = list(np.log1p(y_val['engagement']))\n",
    "y_test = list(np.log1p(y_test['engagement']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-Ydhtfsm48b"
   },
   "outputs": [],
   "source": [
    "#X_train_image = [\"9645.jpg\",\"9646.jpg\"]\n",
    "#X_train_text = X_train_text[0:2]\n",
    "#X_train_source = X_train_source[0:2]\n",
    "#y_train = y_train[0:2]\n",
    "#X_train_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddOhJTTaNew-"
   },
   "source": [
    "## Super model\n",
    "Trying to combine text, image and source_name to form a SuperModel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g4iXNFlpN8WN",
    "outputId": "649d0860-9aaf-42a4-ba83-9050ac921807"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.8.2\n",
      "  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 9.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (21.3)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "\u001b[K     |████████████████████████████████| 880 kB 45.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (4.11.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (3.6.0)\n",
      "Collecting huggingface-hub==0.0.12\n",
      "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (2019.12.20)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (3.13)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (2.23.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 42.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (1.21.6)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers==4.8.2) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.8.2) (3.0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.8.2) (3.8.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2) (2.10)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.8.2) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.8.2) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.8.2) (1.1.0)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=58b5bdb6d48381e96edcdc30e1dd105c36c474eb8481d89a8d0dfd140a1df40d\n",
      "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.8.2\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.12.16-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 7.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.5.11-py2.py3-none-any.whl (144 kB)\n",
      "\u001b[K     |████████████████████████████████| 144 kB 50.8 MB/s \n",
      "\u001b[?25hCollecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 54.3 MB/s \n",
      "\u001b[?25hCollecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=8007abf86be083ed0e370007183a29a8c0065026c40469d24f07af8339941c0a\n",
      "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
      "Successfully built pathtools\n",
      "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
      "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.11 setproctitle-1.2.3 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.16\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers==4.8.2\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqJTx49yNk79"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel, AutoConfig\n",
    "from transformers import EarlyStoppingCallback\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRQER1evbYIC"
   },
   "outputs": [],
   "source": [
    "#Custom model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, drop_rate=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        D_in_image = 10\n",
    "        D_out = 1\n",
    "\n",
    "        #cnn based on images\n",
    "        input_channels = 3\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels,\n",
    "                               out_channels=16,\n",
    "                               kernel_size=5,)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, \n",
    "                               out_channels=32,\n",
    "                               kernel_size=3)\n",
    "        #self.conv2_drop = nn.Dropout2d()\n",
    "        self.l_1_image = nn.Linear(in_features = 32, out_features = 10)\n",
    "        \n",
    "\n",
    "\n",
    "        #dense layer\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.regressor = nn.Linear(D_in_Bert+D_in_source+ D_in_image, D_out) \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        source=None,\n",
    "        image=None,\n",
    "    ):\n",
    "\n",
    "        #image\n",
    "        x = image\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = F.max_pool2d(x,(28,28))\n",
    "        x = x.squeeze()\n",
    "        x = F.relu(self.l_1_image(x))\n",
    "\n",
    "        logits = self.regressor(x)#(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            #  We are doing regression\n",
    "            loss_fct = MSELoss()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            \n",
    "\n",
    "        return (loss, logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lctSh3JeieD"
   },
   "outputs": [],
   "source": [
    "model = CNNModel(drop_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8V4thhekXyf"
   },
   "outputs": [],
   "source": [
    "#Image: Create method that transform image from .jpeg to tensor\n",
    "#We need to resize and normalize picture\n",
    "transform = transforms.Compose([\n",
    "  transforms.Resize((120,120)),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bSZVFhNPD6s"
   },
   "outputs": [],
   "source": [
    "# Create torch dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, source, image, image_transform, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.source = source\n",
    "\n",
    "        self.image_transform = image_transform\n",
    "        self.image_path = image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        img_loc = self.image_path[idx]\n",
    "        image = Image.open(img_loc).convert(\"RGB\")\n",
    "        item[\"image\"] = self.image_transform(image) #transform to tensor, rescale and normalizeW\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.squeeze(torch.tensor(self.labels[idx]))\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "train_dataset = Dataset(X_train_image, transform, y_train)\n",
    "val_dataset = Dataset(X_val_image, transform, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WdUi-5AcPt-i",
    "outputId": "5f153693-350e-4fa1-aa46-3605f18aa610"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "# ----- 2. Fine-tune pretrained model -----#\n",
    "# Define Trainer parameters\n",
    "\n",
    "# Define Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"image\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_steps=1,\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=30,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "    learning_rate = 0.001,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics_reg,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UIpGUOw7PztD",
    "outputId": "cd2528ff-1928-4b02-aa80-48b9c88eebe1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 6278\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11790\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20220508_074148-3cacu8xk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jenspt/huggingface/runs/3cacu8xk\" target=\"_blank\">image</a></strong> to <a href=\"https://wandb.ai/jenspt/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7074' max='11790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7074/11790 1:34:08 < 1:02:46, 1.25 it/s, Epoch 18/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "      <th>Mse</th>\n",
       "      <th>R2</th>\n",
       "      <th>Mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.752400</td>\n",
       "      <td>7.412351</td>\n",
       "      <td>2.722563</td>\n",
       "      <td>7.412351</td>\n",
       "      <td>-0.023498</td>\n",
       "      <td>2.275134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.979600</td>\n",
       "      <td>7.215787</td>\n",
       "      <td>2.686222</td>\n",
       "      <td>7.215787</td>\n",
       "      <td>0.003643</td>\n",
       "      <td>2.240338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.323400</td>\n",
       "      <td>6.786705</td>\n",
       "      <td>2.605130</td>\n",
       "      <td>6.786705</td>\n",
       "      <td>0.062891</td>\n",
       "      <td>2.153294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9.362900</td>\n",
       "      <td>6.629194</td>\n",
       "      <td>2.574722</td>\n",
       "      <td>6.629194</td>\n",
       "      <td>0.084640</td>\n",
       "      <td>2.152106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.804700</td>\n",
       "      <td>6.705364</td>\n",
       "      <td>2.589472</td>\n",
       "      <td>6.705364</td>\n",
       "      <td>0.074123</td>\n",
       "      <td>2.135092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.593700</td>\n",
       "      <td>6.603457</td>\n",
       "      <td>2.569719</td>\n",
       "      <td>6.603457</td>\n",
       "      <td>0.088194</td>\n",
       "      <td>2.124525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.559300</td>\n",
       "      <td>6.626361</td>\n",
       "      <td>2.574172</td>\n",
       "      <td>6.626361</td>\n",
       "      <td>0.085031</td>\n",
       "      <td>2.137789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5.002800</td>\n",
       "      <td>6.595428</td>\n",
       "      <td>2.568156</td>\n",
       "      <td>6.595428</td>\n",
       "      <td>0.089303</td>\n",
       "      <td>2.120844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.331700</td>\n",
       "      <td>6.638323</td>\n",
       "      <td>2.576494</td>\n",
       "      <td>6.638323</td>\n",
       "      <td>0.083380</td>\n",
       "      <td>2.121290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11.321500</td>\n",
       "      <td>7.008771</td>\n",
       "      <td>2.647408</td>\n",
       "      <td>7.008771</td>\n",
       "      <td>0.032228</td>\n",
       "      <td>2.242180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>7.705900</td>\n",
       "      <td>6.612718</td>\n",
       "      <td>2.571521</td>\n",
       "      <td>6.612718</td>\n",
       "      <td>0.086915</td>\n",
       "      <td>2.112022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>7.478000</td>\n",
       "      <td>6.881468</td>\n",
       "      <td>2.623255</td>\n",
       "      <td>6.881469</td>\n",
       "      <td>0.049806</td>\n",
       "      <td>2.218609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>8.566000</td>\n",
       "      <td>6.595215</td>\n",
       "      <td>2.568115</td>\n",
       "      <td>6.595215</td>\n",
       "      <td>0.089332</td>\n",
       "      <td>2.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.199800</td>\n",
       "      <td>6.699690</td>\n",
       "      <td>2.588376</td>\n",
       "      <td>6.699689</td>\n",
       "      <td>0.074906</td>\n",
       "      <td>2.113364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.162500</td>\n",
       "      <td>6.738955</td>\n",
       "      <td>2.595950</td>\n",
       "      <td>6.738955</td>\n",
       "      <td>0.069484</td>\n",
       "      <td>2.165532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>7.030900</td>\n",
       "      <td>6.795848</td>\n",
       "      <td>2.606885</td>\n",
       "      <td>6.795848</td>\n",
       "      <td>0.061629</td>\n",
       "      <td>2.191065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>6.000100</td>\n",
       "      <td>6.654971</td>\n",
       "      <td>2.579723</td>\n",
       "      <td>6.654971</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>2.144561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.762300</td>\n",
       "      <td>6.708470</td>\n",
       "      <td>2.590072</td>\n",
       "      <td>6.708471</td>\n",
       "      <td>0.073694</td>\n",
       "      <td>2.156303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 698\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to image/checkpoint-393\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 698\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to image/checkpoint-786\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 698\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to image/checkpoint-1179\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 698\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to image/checkpoint-1572\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 698\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to image/checkpoint-1965\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 698\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to image/checkpoint-2358\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 698\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to image/checkpoint-2751\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 698\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to image/checkpoint-3144\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 698\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to image/checkpoint-3537\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 698\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to image/checkpoint-3930\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 698\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to image/checkpoint-4323\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 698\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to image/checkpoint-4716\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 698\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to image/checkpoint-5109\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 698\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to image/checkpoint-5502\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 698\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to image/checkpoint-5895\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 698\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to image/checkpoint-6288\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 698\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to image/checkpoint-6681\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 698\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to image/checkpoint-7074\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from image/checkpoint-5109 (score: 6.59521484375).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7074, training_loss=6.6870990285756795, metrics={'train_runtime': 5664.0779, 'train_samples_per_second': 33.252, 'train_steps_per_second': 2.082, 'total_flos': 0.0, 'train_loss': 6.6870990285756795, 'epoch': 18.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train pre-trained model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "di0w4CxuNidN",
    "outputId": "9f545c61-05ea-4b9e-c3e7-0422d65fd78c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1744\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='218' max='218' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [218/218 12:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----- 3. Predict -----#\n",
    "\n",
    "# Create torch dataset\n",
    "test_dataset = Dataset(X_test_image, transform, y_test)\n",
    "\n",
    "# Define test trainer\n",
    "test_trainer = Trainer(model)\n",
    "\n",
    "# Make prediction\n",
    "y_pred, _, _ = test_trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPDT5s3wY9Rv",
    "outputId": "499cd743-c53a-47a1-d48b-4c3889d10af3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 2.105758069301436,\n",
       " 'mse': 6.476038443584958,\n",
       " 'r2': 0.07962411143438064,\n",
       " 'rmse': 2.5448061701404603}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeVzEpSOWr8Q"
   },
   "outputs": [],
   "source": [
    "torch.save(model,\"drive/MyDrive/ABA project/NN_models/image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "It2uIQdLUj4O"
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Image - regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
